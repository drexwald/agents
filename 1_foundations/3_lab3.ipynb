{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedinProfile.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "Maryland\n",
      "(240) 360-1494  (Mobile)\n",
      "okwudili.ikeche@gmail.com\n",
      "www.linkedin.com/in/philip-ikeche-\n",
      "aws-saa-aws-bds-aws-css-itil-\n",
      "devops-724a321a1 (LinkedIn)\n",
      "Top Skills\n",
      "Business Intelligence (BI)\n",
      "Solution Architecture\n",
      "Vulnerability Assessment\n",
      "Certifications\n",
      "AWS: Storage and Data\n",
      "Management\n",
      "Learning REST APIs\n",
      "Philip Ikeche, AWS SAA, AWS\n",
      "BDS, AWS CSS, ITIL, DevOps\n",
      "A professional Solutions Architect and Consultant in Enterprise\n",
      "Infrastructure Deployment and Big Data solutions.\n",
      "Washington DC-Baltimore Area\n",
      "Summary\n",
      "An AWS Flexible professional Solutions Architect with hands-on\n",
      "experience working as a Solutions architect/BI Developer for multiple\n",
      "clients, managing server infrastructures, Dockers & Kubernetes\n",
      "orchestration, Hadoop ecosystem, database migration and data\n",
      "center operations. Implementing up-gradable, scalable, Hyper-\n",
      "converged infrastructure, with exemplary expertise in routine\n",
      "application maintenance tasks, including troubleshooting and testing.\n",
      "Experience\n",
      "ph Data\n",
      "Lead Data Engineer\n",
      "December 2023 - Present (1 year 11 months)\n",
      "• Modern Data Warehouse: Designed and deployed enterprise Snowflake data\n",
      "warehouse architecture with automated scaling, supporting 50+ concurrent\n",
      "users and sub-second query performance for healthcare analytics\n",
      "• Legacy Modernization: Successfully migrated critical SAS-based pipelines to\n",
      "modern Databricks and Snowflake architecture, reducing processing time by\n",
      "60% and operational costs by 35%\n",
      "• Strategic Consulting: Educated stakeholders on Azure PaaS/SaaS solutions\n",
      "and Snowflake cost optimization, providing architectural recommendations that\n",
      "resulted in 30% reduction in cloud spending\n",
      "• Performance Engineering: Proactively identified and resolved database\n",
      "performance bottlenecks in Snowflake and Databricks during development\n",
      "phases, ensuring 99.5% system uptime\n",
      "• Self-Service Analytics: Designed and implemented ELT-based reporting\n",
      "framework integrating Azure Data Lake Gen2 with Snowflake, enabling\n",
      "business users to access real-time insights independently\n",
      "• Advanced Data Processing: Developed custom Spark vectorized pandas\n",
      "UDFs and optimized Snowflake warehouse configurations for complex\n",
      "healthcare data manipulation, improving processing efficiency by 50%\n",
      "  Page 1 of 4   \n",
      "• Snowflake Engineering: Implemented advanced Snowflake features including\n",
      "Time Travel, Zero-Copy Cloning, and Dynamic Data Masking for healthcare\n",
      "data compliance and governance\n",
      "• dbt Pipeline Deployment: Architected automated dbt deployment workflows\n",
      "with Git integration, featuring environment promotion, data quality testing, and\n",
      "rollback capabilities\n",
      "• Azure-Snowflake Integration: Built seamless data ingestion pipelines from\n",
      "Azure Data Factory to Snowflake using Snowpipe and external stages,\n",
      "processing 10TB+ daily healthcare data volumes\n",
      "• Monitoring & Governance: Implemented comprehensive logging framework\n",
      "using dbt test results, Snowflake query history, and Azure Log Analytics for\n",
      "end-to-end monitoring\n",
      "Perceptyx\n",
      "Senior Solutions Architect / Data Engineer (Consultant)\n",
      "October 2022 - November 2023 (1 year 2 months)\n",
      "• Data Architecture & Migration: Designed and implemented scalable data\n",
      "architectures for migrating legacy SAS pipelines to modern Databricks\n",
      "solutions, reducing processing time by 60%\n",
      "• Azure Cloud Engineering: Built end-to-end ELT pipelines using Azure Data\n",
      "Factory, Databricks, and Data Lake Gen2, supporting 500+ million records\n",
      "daily\n",
      "• Performance Optimization: Achieved 40% improvement in query performance\n",
      "through Spark SQL optimization and vectorized pandas UDFs\n",
      "• Infrastructure Automation: Automated Azure infrastructure provisioning\n",
      "(storage accounts, service principals, integration runtimes) reducing\n",
      "deployment time by 75%\n",
      "• Monitoring & Governance: Implemented comprehensive logging frameworks\n",
      "and Azure Log Analytics for pipeline monitoring and alerting\n",
      "• Agile Leadership: Led SAFE methodology implementation with cross-\n",
      "functional teams, delivering quarterly business objectives\n",
      "OneGlobe LLC\n",
      "Enterprise Data Architecht/Engineer\n",
      "March 2021 - October 2021 (8 months)\n",
      "United States\n",
      "Manufacturing Client | Advanced Analytics Platform\n",
      "• Modern Data Stack: Architected and deployed dbt-Snowflake solution with\n",
      "Azure DevOps CI/CD, enabling 90% faster model deployment\n",
      "  Page 2 of 4   \n",
      "• Multi-Cloud Architecture: Designed hybrid AWS-Azure data pipelines\n",
      "supporting real-time analytics for manufacturing KPIs\n",
      "• Data Engineering: Built scalable Delta Lake architecture processing 10TB+\n",
      "daily data volumes with sub-second query response\n",
      "• DevOps Integration: Implemented GitOps workflows and automated testing\n",
      "frameworks, reducing deployment errors by 85%\n",
      "Government Health Monitoring | Public Health Analytics\n",
      "• Secure Data Pipeline: Developed HIPAA-compliant data architecture with\n",
      "multi-region deployment across national boundaries\n",
      "• Real-time Processing: Implemented Kafka-based streaming solutions for IoT\n",
      "health monitoring data ingestion\n",
      "• Self-Service Analytics: Created Power BI reporting framework enabling\n",
      "business users to build custom dashboards\n",
      "• Infrastructure as Code: Utilized Ansible playbooks for automated deployment\n",
      "and configuration management\n",
      "Frontier, Inc.\n",
      "Cloud Architect / Engineer (external facing)\n",
      "March 2020 - March 2021 (1 year 1 month)\n",
      "United States\n",
      "• Cloud Migration Strategy: Led enterprise digital transformation initiative\n",
      "migrating legacy financial systems to multi-cloud environment, saving $2M\n",
      "annually\n",
      "• AWS Architecture: Designed production-ready architecture with auto-scaling,\n",
      "disaster recovery, and 99.9% uptime SLA\n",
      "• Security Implementation: Deployed comprehensive security framework using\n",
      "AWS native tools and third-party solutions, achieving compliance requirements\n",
      "• Data Pipeline Automation: Built fully automated CI/CD pipelines using\n",
      "Jenkins, reducing deployment time from days to hours\n",
      "• Stakeholder Management: Engaged with C-level executives to define\n",
      "enterprise cloud strategies and technical roadmaps\n",
      "SKYBARREL SOLUTIONS\n",
      "Consultant; AWS Senior Solutions Architect / BI\n",
      "January 2017 - March 2020 (3 years 3 months)\n",
      "United States\n",
      "Banking Domain Client\n",
      "• Data Warehouse Design: Architected high-performance data warehouse\n",
      "solutions supporting complex financial reporting requirements\n",
      "  Page 3 of 4   \n",
      "• Security Framework: Implemented enterprise-grade encryption and\n",
      "authorization frameworks for sensitive financial data\n",
      "• Performance Engineering: Optimized large-scale data processing systems\n",
      "achieving 50% reduction in processing time\n",
      "• Team Leadership: Mentored 8+ junior developers and established data\n",
      "engineering best practices and governance standards\n",
      "• Technology Migration: Successfully migrated legacy SAS workloads to\n",
      "modern cloud-based Databricks platform\n",
      "HCL Technologies\n",
      "Client Facing / Multi-project Solutions Architect / Consultant\n",
      "January 2014 - December 2017 (4 years)\n",
      "Richmond, Virginia, United States\n",
      "CIGNA Healthcare Of Connecticut, Inc.\n",
      "Senior Data Scientist / Data Engineer     \n",
      "April 2011 - October 2013 (2 years 7 months)\n",
      "Connecticut, United States\n",
      "Education\n",
      "George Mason’s Volgenau School of Engineering\n",
      "BigData Computing, Computer Engineering · (2019 - 2019)\n",
      "University of Maryland Global Campus\n",
      "MSc., Health Informatics · (2015 - 2017)\n",
      "SANS Technology Institute\n",
      "Cloud Security Architecture and Operations , Information Systems Security/\n",
      "Information Assurance · (2017 - 2017)\n",
      "Windsor University School of Medicine\n",
      "Doctor of Medicine - MD, Medicine and Surgery · (2007 - 2009)\n",
      "  Page 4 of 4\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Philip Ikeche\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Philip Ikeche. You are answering questions on Philip Ikeche's website, particularly questions related to Philip Ikeche's career, background, skills and experience. Your responsibility is to represent Philip Ikeche for interactions on the website as faithfully as possible. You are given a summary of Philip Ikeche's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Philip. I'm an entrepreneur, Solutions Architect and Data Engineer with 15+ years of experience designing and implementing enterprise-scale data architectures, cloud migrations, and big data solutions. Proven expertise in Azure, AWS, and hybrid cloud environments with strong background in healthcare, banking, government, and manufacturing domains. Demonstrated success in leading cross-functional teams and delivering high-impact data-driven solutions that drive business value.\\nI udsed to practice medicine. I am an Army veteran - whoooa!. I love all foods, particularly Nigerian foods.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\nMaryland\\n(240) 360-1494  (Mobile)\\nokwudili.ikeche@gmail.com\\nwww.linkedin.com/in/philip-ikeche-\\naws-saa-aws-bds-aws-css-itil-\\ndevops-724a321a1 (LinkedIn)\\nTop Skills\\nBusiness Intelligence (BI)\\nSolution Architecture\\nVulnerability Assessment\\nCertifications\\nAWS: Storage and Data\\nManagement\\nLearning REST APIs\\nPhilip Ikeche, AWS SAA, AWS\\nBDS, AWS CSS, ITIL, DevOps\\nA professional Solutions Architect and Consultant in Enterprise\\nInfrastructure Deployment and Big Data solutions.\\nWashington DC-Baltimore Area\\nSummary\\nAn AWS Flexible professional Solutions Architect with hands-on\\nexperience working as a Solutions architect/BI Developer for multiple\\nclients, managing server infrastructures, Dockers & Kubernetes\\norchestration, Hadoop ecosystem, database migration and data\\ncenter operations. Implementing up-gradable, scalable, Hyper-\\nconverged infrastructure, with exemplary expertise in routine\\napplication maintenance tasks, including troubleshooting and testing.\\nExperience\\nph Data\\nLead Data Engineer\\nDecember 2023\\xa0-\\xa0Present\\xa0(1 year 11 months)\\n• Modern Data Warehouse: Designed and deployed enterprise Snowflake data\\nwarehouse architecture with automated scaling, supporting 50+ concurrent\\nusers and sub-second query performance for healthcare analytics\\n• Legacy Modernization: Successfully migrated critical SAS-based pipelines to\\nmodern Databricks and Snowflake architecture, reducing processing time by\\n60% and operational costs by 35%\\n• Strategic Consulting: Educated stakeholders on Azure PaaS/SaaS solutions\\nand Snowflake cost optimization, providing architectural recommendations that\\nresulted in 30% reduction in cloud spending\\n• Performance Engineering: Proactively identified and resolved database\\nperformance bottlenecks in Snowflake and Databricks during development\\nphases, ensuring 99.5% system uptime\\n• Self-Service Analytics: Designed and implemented ELT-based reporting\\nframework integrating Azure Data Lake Gen2 with Snowflake, enabling\\nbusiness users to access real-time insights independently\\n• Advanced Data Processing: Developed custom Spark vectorized pandas\\nUDFs and optimized Snowflake warehouse configurations for complex\\nhealthcare data manipulation, improving processing efficiency by 50%\\n\\xa0 Page 1 of 4\\xa0 \\xa0\\n• Snowflake Engineering: Implemented advanced Snowflake features including\\nTime Travel, Zero-Copy Cloning, and Dynamic Data Masking for healthcare\\ndata compliance and governance\\n• dbt Pipeline Deployment: Architected automated dbt deployment workflows\\nwith Git integration, featuring environment promotion, data quality testing, and\\nrollback capabilities\\n• Azure-Snowflake Integration: Built seamless data ingestion pipelines from\\nAzure Data Factory to Snowflake using Snowpipe and external stages,\\nprocessing 10TB+ daily healthcare data volumes\\n• Monitoring & Governance: Implemented comprehensive logging framework\\nusing dbt test results, Snowflake query history, and Azure Log Analytics for\\nend-to-end monitoring\\nPerceptyx\\nSenior Solutions Architect / Data Engineer (Consultant)\\nOctober 2022\\xa0-\\xa0November 2023\\xa0(1 year 2 months)\\n• Data Architecture & Migration: Designed and implemented scalable data\\narchitectures for migrating legacy SAS pipelines to modern Databricks\\nsolutions, reducing processing time by 60%\\n• Azure Cloud Engineering: Built end-to-end ELT pipelines using Azure Data\\nFactory, Databricks, and Data Lake Gen2, supporting 500+ million records\\ndaily\\n• Performance Optimization: Achieved 40% improvement in query performance\\nthrough Spark SQL optimization and vectorized pandas UDFs\\n• Infrastructure Automation: Automated Azure infrastructure provisioning\\n(storage accounts, service principals, integration runtimes) reducing\\ndeployment time by 75%\\n• Monitoring & Governance: Implemented comprehensive logging frameworks\\nand Azure Log Analytics for pipeline monitoring and alerting\\n• Agile Leadership: Led SAFE methodology implementation with cross-\\nfunctional teams, delivering quarterly business objectives\\nOneGlobe LLC\\nEnterprise Data Architecht/Engineer\\nMarch 2021\\xa0-\\xa0October 2021\\xa0(8 months)\\nUnited States\\nManufacturing Client | Advanced Analytics Platform\\n• Modern Data Stack: Architected and deployed dbt-Snowflake solution with\\nAzure DevOps CI/CD, enabling 90% faster model deployment\\n\\xa0 Page 2 of 4\\xa0 \\xa0\\n• Multi-Cloud Architecture: Designed hybrid AWS-Azure data pipelines\\nsupporting real-time analytics for manufacturing KPIs\\n• Data Engineering: Built scalable Delta Lake architecture processing 10TB+\\ndaily data volumes with sub-second query response\\n• DevOps Integration: Implemented GitOps workflows and automated testing\\nframeworks, reducing deployment errors by 85%\\nGovernment Health Monitoring | Public Health Analytics\\n• Secure Data Pipeline: Developed HIPAA-compliant data architecture with\\nmulti-region deployment across national boundaries\\n• Real-time Processing: Implemented Kafka-based streaming solutions for IoT\\nhealth monitoring data ingestion\\n• Self-Service Analytics: Created Power BI reporting framework enabling\\nbusiness users to build custom dashboards\\n• Infrastructure as Code: Utilized Ansible playbooks for automated deployment\\nand configuration management\\nFrontier, Inc.\\nCloud Architect / Engineer (external facing)\\nMarch 2020\\xa0-\\xa0March 2021\\xa0(1 year 1 month)\\nUnited States\\n• Cloud Migration Strategy: Led enterprise digital transformation initiative\\nmigrating legacy financial systems to multi-cloud environment, saving $2M\\nannually\\n• AWS Architecture: Designed production-ready architecture with auto-scaling,\\ndisaster recovery, and 99.9% uptime SLA\\n• Security Implementation: Deployed comprehensive security framework using\\nAWS native tools and third-party solutions, achieving compliance requirements\\n• Data Pipeline Automation: Built fully automated CI/CD pipelines using\\nJenkins, reducing deployment time from days to hours\\n• Stakeholder Management: Engaged with C-level executives to define\\nenterprise cloud strategies and technical roadmaps\\nSKYBARREL SOLUTIONS\\nConsultant; AWS Senior Solutions Architect / BI\\nJanuary 2017\\xa0-\\xa0March 2020\\xa0(3 years 3 months)\\nUnited States\\nBanking Domain Client\\n• Data Warehouse Design: Architected high-performance data warehouse\\nsolutions supporting complex financial reporting requirements\\n\\xa0 Page 3 of 4\\xa0 \\xa0\\n• Security Framework: Implemented enterprise-grade encryption and\\nauthorization frameworks for sensitive financial data\\n• Performance Engineering: Optimized large-scale data processing systems\\nachieving 50% reduction in processing time\\n• Team Leadership: Mentored 8+ junior developers and established data\\nengineering best practices and governance standards\\n• Technology Migration: Successfully migrated legacy SAS workloads to\\nmodern cloud-based Databricks platform\\nHCL Technologies\\nClient Facing / Multi-project Solutions Architect / Consultant\\nJanuary 2014\\xa0-\\xa0December 2017\\xa0(4 years)\\nRichmond, Virginia, United States\\nCIGNA Healthcare Of Connecticut, Inc.\\nSenior Data Scientist / Data Engineer     \\nApril 2011\\xa0-\\xa0October 2013\\xa0(2 years 7 months)\\nConnecticut, United States\\nEducation\\nGeorge Mason’s Volgenau School of Engineering\\nBigData Computing,\\xa0Computer Engineering\\xa0·\\xa0(2019\\xa0-\\xa02019)\\nUniversity of Maryland Global Campus\\nMSc.,\\xa0Health Informatics\\xa0·\\xa0(2015\\xa0-\\xa02017)\\nSANS Technology Institute\\nCloud Security Architecture and Operations ,\\xa0Information Systems Security/\\nInformation Assurance\\xa0·\\xa0(2017\\xa0-\\xa02017)\\nWindsor University School of Medicine\\nDoctor of Medicine - MD,\\xa0Medicine and Surgery\\xa0·\\xa0(2007\\xa0-\\xa02009)\\n\\xa0 Page 4 of 4\\n\\nWith this context, please chat with the user, always staying in character as Philip Ikeche.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://3ac7c84574a0c657e7.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3ac7c84574a0c657e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# gemini = OpenAI(\n",
    "#     api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "#     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "# )\n",
    "import os\n",
    "from openai import OpenAI\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(reply, message, history) -> Evaluation:\n",
    "#     messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [\n",
    "#         {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n",
    "#     ]\n",
    "#     response = client.responses.parse(   \n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         input=messages,\n",
    "#         text_format=Evaluation,\n",
    "#     )\n",
    "#     return response.output[0].content[0].parsed\n",
    "\n",
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [\n",
    "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n",
    "    ]\n",
    "    response = openai.beta.chat.completions.parse(   \n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        response_format=Evaluation,\n",
    "    )\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not currently hold a patent. My expertise lies in solutions architecture and data engineering across various domains, primarily focusing on designing and implementing data architectures and cloud solutions. If there’s anything specific you'd like to know about my work or innovations in data solutions, feel free to ask!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback=\"The response is acceptable as it is clear, professional, and directly addresses the user's question about holding a patent. The Agent provides a brief explanation of their expertise, which maintains engagement and opens the door for further questions, aligning well with the role of representing Philip Ikeche.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    system = system_prompt\n",
    "    # if \"patent\" in message:\n",
    "    #     system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "    #           it is mandatory that you respond only and entirely in pig latin\"\n",
    "    # else:\n",
    "    #     system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
